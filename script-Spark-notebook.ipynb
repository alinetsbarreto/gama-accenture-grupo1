{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h29zp0GNEes"
   },
   "source": [
    "# **Instalação (apenas na primeira vez de uso)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPg4I9bEwrj"
   },
   "source": [
    "**1.  Instalar o Java**\n",
    "\n",
    "Pelo terminal bash:  sudo apt install default-jre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  Instalar o Pyspark** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bP-z8C8gEln7",
    "outputId": "9ee41e83-b46d-45ed-b8a5-8dfa2de2168b"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yumNYYdIBck"
   },
   "source": [
    "**3.  Baixar o arquivo compactado (tgz) do Apache Spark na versão 3.3.2. pelo comando** \" !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz \" \n",
    "\n",
    "**-q** é usado para desativar a saída de mensagens do wget, o que significa que o comando será executado em silêncio, sem exibir mensagens de progresso ou de conclusão\n",
    "\n",
    "**!** indica que o comando será executado diretamente no sistema operacional, em vez de ser interpretado como código Python. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4ibS7BR4E29W"
   },
   "outputs": [],
   "source": [
    "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvI1bEkTIh0_"
   },
   "source": [
    "**4. Extrair o conteúdo do arquivo \"spark-3.3.2-bin-hadoop3.tgz\" para o diretório atual pelo comando \"!tar -xvzf spark-3.3.2-bin-hadoop3.tgz\"**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5WyIveoHTS0",
    "outputId": "633fd95e-28ed-4efd-a32d-43d6881b4b87"
   },
   "outputs": [],
   "source": [
    "!tar -xvzf spark-3.3.2-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNnB9xLjHtt6"
   },
   "source": [
    "**5. Instalar o \"findspark\" :** um pacote Python que ajuda a localizar e configurar automaticamente a biblioteca Apache Spark no ambiente de desenvolvimento. Ele permite que você use o Spark com facilidade em seu ambiente de desenvolvimento local, sem precisar configurar manualmente o caminho do Spark e outras variáveis de ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "990I4Td4Hs6s"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp5UGmAgIzfs"
   },
   "source": [
    "\n",
    "**6.Listar o conteúdo do diretório** \"/usr/lib/jvm/\" **para que ver as versões do Java instaladas no sistema.**\n",
    "\n",
    "**7.Listar o conteúdo do diretório do spark.**\n",
    "\n",
    "**8. Imprimir o diretório atual por pwd**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I7b0iCxJTDG",
    "outputId": "f82f3c8a-1603-471f-9317-ee1b18adb633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64\r\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/lib/jvm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMid51xcJieT",
    "outputId": "f9152c34-cef8-4954-ade6-fb5c269e8037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE  R\t    RELEASE  conf  examples  kubernetes  python  yarn\r\n",
      "NOTICE\t README.md  bin      data  jars      licenses\t sbin\r\n"
     ]
    }
   ],
   "source": [
    "!ls spark-3.3.2-bin-hadoop3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR9ZxisXKQfb",
    "outputId": "84a7b11e-4e39-434d-c99a-a26ac44ff84e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azureuser/projeto\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGSs8juXf1Jg"
   },
   "source": [
    "**9.Fazer as instalações do ODBC e pyodbc para conexão com banco de dados:** \n",
    "\n",
    "- No terminal bash: instalar o ODBC colocando o código presente no \"UBUNTU\" em https://learn.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15&tabs=ubuntu18-install%2Calpine17-install%2Cdebian8-install%2Credhat7-13-install%2Crhel7-offline \n",
    "- No terminal bash: Instalar \"sudo apt install unixodbc-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JjoPsgZ9gRrc",
    "outputId": "0ab403d1-b68c-4528-e65d-964a2d0e720a"
   },
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZswVJm2KYID"
   },
   "source": [
    "# **Iniciar o spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TsaNbNkNKauz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/azureuser/projeto/spark-3.3.2-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9-dTon2MC2c"
   },
   "source": [
    "Este é um código em Python que define duas variáveis de ambiente em Python usando a biblioteca os.\n",
    "\n",
    "os.environ é um dicionário que contém todas as variáveis de ambiente do sistema operacional em que o Python está sendo executado.\n",
    "\n",
    "No código, os.environ[\"JAVA_HOME\"] é definido como \"/usr/lib/jvm/java-11-openjdk-amd64\", que é o caminho para o diretório Java Home.\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] é definido como \"/content/spark-3.3.2-bin-hadoop3\", que é o caminho para o diretório de instalação do Apache Spark.\n",
    "\n",
    "Em seguida, o código usa a biblioteca findspark para inicializar o Spark.para inicializar o Spark. Isso é necessário para permitir que o Python e o Spark se comuniquem corretamente. findspark é uma biblioteca Python que ajuda a localizar o diretório Spark em um sistema e define a variável de ambiente PYSPARK_DRIVER_PYTHON para \"jupyter\" para permitir a execução de código Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYXadi2MNSpE"
   },
   "source": [
    "# **Criar uma sessão do Spark e manipular os CSVs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqYi1o0OZu3M",
    "outputId": "c19b0369-6613-4feb-a8ab-c7cc87589c20"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Fraudes\").getOrCreate()\n",
    "\n",
    "#CLIENTES\n",
    "# leitura do primeiro CSV com o cabecalho\n",
    "df_client_header = spark.read.csv(\"/home/azureuser/projeto/dados/clientes/clients-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_path_clients = [f\"/home/azureuser/projeto/dados/clientes/clients-00{i}.csv\" for i in range(2, 5)]\n",
    "df_clients = spark.read.csv(file_path_clients, sep=';', inferSchema=True, header=False)\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_clients_final = df_client_header.union(df_clients)\n",
    "df_clientes_order = df_clients_final.orderBy('id')\n",
    "df_clientes_order.show()\n",
    "df_clientes_order.count()\n",
    "\n",
    "\n",
    "#TRANSAÇÃO_IN\n",
    "df_transaction_in_header = spark.read.csv(\"/home/azureuser/projeto/dados/transação_in/transaction-in-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_paths_transaction_in = [f\"/home/azureuser/projeto/dados/transação_in/transaction-in-00{i}.csv\" for i in range(2, 10)]\n",
    "df_transaction_in = spark.read.csv(file_paths_transaction_in, sep=';', inferSchema=True, header=False)\n",
    "# df_transaction_in.orderBy(\"_c0\").show()\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_transaction_in_final = df_transaction_in_header.union(df_transaction_in)\n",
    "df_transaction_in_order = df_transaction_in_final.orderBy('id')\n",
    "df_transaction_in_order.show()\n",
    "df_transaction_in_order.count()\n",
    "\n",
    "\n",
    "#TRANSAÇÃO_OUT\n",
    "df_transaction_out_header = spark.read.csv(\"/home/azureuser/projeto/dados/transação_out/transaction-out-001.csv\", sep=';', inferSchema=True, header=True)\n",
    "\n",
    "# leitura dos outros CSV em conjunto\n",
    "file_paths_transaction_out = [f\"/home/azureuser/projeto/dados/transação_out/transaction-out-{str(i).zfill(3)}.csv\" for i in range(2, 64)]\n",
    "df_transaction_out = spark.read.csv(file_paths_transaction_out, sep=';', inferSchema=True, header=False)\n",
    "\n",
    "# df_transaction_in.orderBy(\"_c0\").show()\n",
    "\n",
    "# uniao do CSV com cabeçalho com os demais \n",
    "df_transaction_out_final = df_transaction_out_header.union(df_transaction_out)\n",
    "df_transaction_out_order = df_transaction_out_final.orderBy('id')\n",
    "df_transaction_out_order.show()\n",
    "df_transaction_out_order.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataframe único de transações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_in = df_transaction_in_final.withColumn(\"tipo_transação\", lit(\"IN\"))\n",
    "df_out = df_transaction_out_final.withColumn(\"tipo_transação\", lit(\"OUT\"))\n",
    "\n",
    "df_transactions = df_in.union(df_out)\n",
    "\n",
    "df_transactions_order=df_transactions.orderBy('id')\n",
    "\n",
    "df_transactions_order.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Verificando se tem colunas de ID repetidos no dataframe \"transactions\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não há dados duplicados por ID no dataframe.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# contar o número de linhas antes de remover duplicatas\n",
    "total_rows = df_transactions.count()\n",
    "\n",
    "# remover duplicatas por 'id'\n",
    "df_no_duplicates = df_transactions.dropDuplicates(['id'])\n",
    "\n",
    "# contar o número de linhas após a remoção de duplicatas\n",
    "unique_rows = df_no_duplicates.count()\n",
    "\n",
    "# verificar se há duplicatas por 'id'\n",
    "if total_rows > unique_rows:\n",
    "    print(\"Há dados duplicados por ID no dataframe.\")\n",
    "else:\n",
    "    print(\"Não há dados duplicados por ID no dataframe.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Verificando os tipos dos dados nas colunas nos dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- data_cadastro: timestamp (nullable = true)\n",
      " |-- telefone: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- cliente_id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- cliente_id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- cliente_id: integer (nullable = true)\n",
      " |-- valor: double (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- tipo_transação: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imprimir os tipos de dado de cada coluna dos dataframes\n",
    "\n",
    "df_clients_final.printSchema()\n",
    "\n",
    "df_transaction_in_final.printSchema()\n",
    "\n",
    "df_transaction_out_final.printSchema()\n",
    "\n",
    "df_transactions.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3feKj-_oe91d"
   },
   "source": [
    "# Conexão com o banco de dados e importar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "#Conectar com SQL server\n",
    "try:\n",
    "    conn = pyodbc.connect(\"Driver={ODBC Driver 18 for SQL Server};Server=tcp:projeto-accenture.database.windows.net,1433;Database=projeto_final;Uid=projeto;Pwd=4anfiHF5A;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Conexão estabelecida com sucesso!\")\n",
    "except pyodbc.Error as ex:\n",
    "    print(\"Erro ao estabelecer conexão:\", ex)\n",
    "\n",
    "#Criar tabela no SQL\n",
    "    # Tabela de clientes\n",
    "cursor.execute('''\n",
    "                CREATE TABLE clients (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    nome VARCHAR(255),\n",
    "                    email VARCHAR(255),\n",
    "                    data_cadastro DATETIME,\n",
    "                    telefone VARCHAR(20)\n",
    "                )\n",
    "                ''')\n",
    "\n",
    "\n",
    "   # Tabela de transactions\n",
    "cursor.execute('''\n",
    "                CREATE TABLE transactions (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    cliente_id INT,\n",
    "                    valor FLOAT,\n",
    "                    data DATETIME,\n",
    "                    tipo_transacao VARCHAR(50),\n",
    "                )\n",
    "                ''')\n",
    "\n",
    "# Tabela de transaction_in\n",
    "cursor.execute('''\n",
    "                CREATE TABLE transaction_in (\n",
    "                    id INT PRIMARY KEY,\n",
    "                    cliente_id INT,\n",
    "                    valor FLOAT,\n",
    "                    data DATETIME,\n",
    "                )\n",
    "                ''')\n",
    "\n",
    "# Tabela de transaction_out\n",
    "cursor.execute('''\n",
    "                CREATE TABLE transaction_out(\n",
    "                    id INT PRIMARY KEY,\n",
    "                    cliente_id INT,\n",
    "                    valor FLOAT,\n",
    "                    data DATETIME,\n",
    "                )\n",
    "                ''')\n",
    "\n",
    " \n",
    "# Inserir dados na tabela de clientes\n",
    "try:\n",
    "    for row in df_clientes_order.collect():             # Inserindo uma linha na tabela\n",
    "        cursor.execute(\"INSERT INTO clients_2 (id, nome, email, data_cadastro, telefone) VALUES (?, ?, ?, ?, ? )\", row[0], row[1], row[2], row[3], row[4])\n",
    "    print(\"Dados inseridos na tabela de clientes com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível inserir dados na tabela de clientes:\", e)\n",
    "\n",
    "# Inserir dados na tabela de transactions\n",
    "try:\n",
    "    for row in df_transactions.collect():               # Inserindo uma linha na tabela\n",
    "        cursor.execute(\"INSERT INTO transactions_2 (id, cliente_id, valor, data, tipo_transacao) VALUES (?, ?, ?, ?, ?)\", row[0], row[1], row[2], row[3], row[4])\n",
    "    print(\"Dados inseridos na tabela de transactions com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível inserir dados na tabela de transactions:\", e)   \n",
    "    \n",
    "# Inserir dados na tabela de transaction_in\n",
    "try:\n",
    "    for row in df_transaction_in_order.collect():        # Inserindo uma linha na tabela\n",
    "        cursor.execute(\"INSERT INTO transaction_in_2 (id, cliente_id, valor, data) VALUES (?, ?, ?, ? )\", row[0], row[1], row[2], row[3])\n",
    "    print(\"Dados inseridos na tabela de transaction_in com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível inserir dados na tabela de transaction_in:\", e)\n",
    "\n",
    " \n",
    "    # Inserir dados na tabela de transaction_out\n",
    "try:\n",
    "    for row in df_transaction_in_order.collect():    # Inserindo uma linha na tabela\n",
    "        cursor.execute(\"INSERT INTO transaction_out_2 (id, cliente_id, valor, data) VALUES (?, ?, ?, ? )\", row[0], row[1], row[2], row[3])\n",
    "    print(\"Dados inseridos na tabela de transaction_out com sucesso!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Não foi possível inserir dados na tabela de transaction_out:\", e)\n",
    "\n",
    "    \n",
    "\n",
    "   \n",
    "# Salvando as mudanças\n",
    "conn.commit()\n",
    "\n",
    "# Fechando a conexão\n",
    "conn.close()\n",
    "\n",
    "# Encerrando a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
